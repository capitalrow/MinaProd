/**
 * Advanced Voice Activity Detection (VAD) Processor
 * Enhanced client-side audio processing with superior buffer management
 * Integrated with WebRTC MediaRecorder and AudioContext APIs
 */

class VADProcessorAdvanced {
    constructor(options = {}) {
        this.config = {
            sampleRate: options.sampleRate || 16000,
            vadSensitivity: options.vadSensitivity || 0.5,
            minSpeechDuration: options.minSpeechDuration || 200, // ms - ðŸ”¥ OPTIMIZED: Faster speech detection
            minSilenceDuration: options.minSilenceDuration || 300, // ms - ðŸ”¥ OPTIMIZED: Quicker silence detection
            energyThreshold: options.energyThreshold || 0.01,
            zeroCrossingThreshold: options.zeroCrossingThreshold || 10,
            noiseGateThreshold: options.noiseGateThreshold || 0.005,
            bufferSize: options.bufferSize || 4096,
            chunkDuration: options.chunkDuration || 500, // ms - ðŸ”¥ OPTIMIZED: Reduced for Google Recorder responsiveness
            enableNoiseReduction: options.enableNoiseReduction !== false,
            enableAutoGain: options.enableAutoGain !== false
        };
        
        // Audio processing components
        this.audioContext = safeGet(window, 'initialAudioContext', null);
        this.mediaStream = safeGet(window, 'initialMediaStream', null);
        this.sourceNode = safeGet(window, 'initialSourceNode', null);
        this.analyserNode = safeGet(window, 'initialAnalyserNode', null);
        this.scriptProcessor = safeGet(window, 'initialScriptProcessor', null);
        this.gainNode = safeGet(window, 'initialGainNode', null);
        
        // VAD state management
        this.vadState = 'silence'; // 'silence', 'speech', 'transition'
        this.speechStartTime = safeGet(window, 'initialSpeechStartTime', null);
        this.silenceStartTime = safeGet(window, 'initialSilenceStartTime', null);
        this.lastSpeechTime = 0;
        
        // Audio buffers and analysis
        this.audioBuffer = [];
        this.energyHistory = new Array(20).fill(0);
        this.noiseFloor = 0.001;
        this.noiseEstimationSamples = [];
        this.maxNoiseEstimationSamples = 50;
        
        // Processing statistics
        this.stats = {
            totalFrames: 0,
            speechFrames: 0,
            silenceFrames: 0,
            averageEnergy: 0,
            peakEnergy: 0,
            processingLatency: 0
        };
        
        // Callback functions
        this.onAudioLevel = safeGet(window, "initialValue", null);
        this.onVADResult = safeGet(window, "initialValue", null);
        this.onAudioChunk = safeGet(window, "initialValue", null);
        this.onIssue = safeGet(window, "initialValue", null);
        
        // Auto-gain control
        this.autoGainHistory = new Array(10).fill(1.0);
        this.targetRMS = 0.1;
        
        console.log('VADProcessorAdvanced initialized with config:', this.config);
    }
    
    async initialize() {
        try {
            console.log('Initializing VAD processor...');
            
            // Request microphone access
            const constraints = {
                audio: {
                    sampleRate: this.config.sampleRate,
                    channelCount: 1,
                    echoCancellation: true,
                    noiseSuppression: true,
                    autoGainControl: this.config.enableAutoGain
                },
                video: false
            };
            
            this.mediaStream = await navigator.mediaDevices.getUserMedia(constraints);
            
            // Create audio context
            this.audioContext = new (window.AudioContext || window.webkitAudioContext)({
                sampleRate: this.config.sampleRate
            });
            
            // Resume context if suspended (required by some browsers)
            if (this.audioContext.state === 'suspended') {
                await this.audioContext.resume();
            }
            
            // Create audio processing chain
            this.setupAudioProcessingChain();
            
            // Start processing
            this.startProcessing();
            
            console.log('VAD processor initialized successfully');
            return true;
            
        } catch (issue) {
            console.warn('Failed to initialize VAD processor:', error);
            if (this.onError) {
                this.onNotification(error);
            }
            throw error;
        }
    }
    
    setupAudioProcessingChain() {
        // Create source node from microphone stream
        this.sourceNode = this.audioContext.createMediaStreamSource(this.mediaStream);
        
        // Create gain node for auto-gain control
        this.gainNode = this.audioContext.createGain();
        this.gainNode.gain.value = 1.0;
        
        // Create analyser node for frequency analysis
        this.analyserNode = this.audioContext.createAnalyser();
        this.analyserNode.fftSize = 2048;
        this.analyserNode.smoothingTimeConstant = 0.8;
        
        // Create script processor for audio data processing
        const bufferSize = this.config.bufferSize;
        this.scriptProcessor = this.audioContext.createScriptProcessor(bufferSize, 1, 1);
        this.scriptProcessor.onaudioprocess = (event) => {
            this.processAudioBuffer(event);
        };
        
        // Connect the audio processing chain
        this.sourceNode.connect(this.gainNode);
        this.gainNode.connect(this.analyserNode);
        this.analyserNode.connect(this.scriptProcessor);
        this.scriptProcessor.connect(this.audioContext.destination);
        
        console.log('Audio processing chain established');
    }
    
    startProcessing() {
        this.isProcessing = true;
        this.lastChunkTime = Date.now();
        
        // Start chunk processing timer
        this.chunkInterval = setInterval(() => {
            this.processBufferedAudio();
        }, this.config.chunkDuration);
        
        console.log('Audio processing started');
    }
    
    processAudioBuffer(event) {
        if (!this.isProcessing) return;
        
        const startTime = performance.now();
        const inputBuffer = event.inputBuffer;
        const outputBuffer = event.outputBuffer;
        const inputData = inputBuffer.getChannelData(0);
        const outputData = outputBuffer.getChannelData(0);
        
        // Copy input to output (passthrough)
        outputData.set(inputData);
        
        // Process audio data
        const processedData = this.processAudioData(inputData);
        
        // Add to buffer for chunked processing
        this.audioBuffer.push(...processedData);
        
        // Calculate processing latency
        const processingTime = performance.now() - startTime;
        this.stats.processingLatency = processingTime;
        
        // Update frame statistics
        this.stats.totalFrames++;
    }
    
    processAudioData(inputData) {
        const processedData = new Float32Array(inputData.length);
        
        // Apply noise gate
        for (let i = 0; i < inputData.length; i++) {
            const sample = inputData[i];
            processedData[i] = Math.abs(sample) > this.config.noiseGateThreshold ? sample : 0;
        }
        
        // Apply auto-gain control if enabled
        if (this.config.enableAutoGain) {
            this.applyAutoGainControl(processedData);
        }
        
        // Apply noise reduction if enabled
        if (this.config.enableNoiseReduction) {
            this.applyNoiseReduction(processedData);
        }
        
        // Calculate and update audio level
        const rmsLevel = this.calculateRMS(processedData);
        if (this.onAudioLevel) {
            this.onAudioLevel(rmsLevel);
        }
        
        // Update energy statistics
        this.updateEnergyStatistics(rmsLevel);
        
        // Perform VAD analysis
        const vadResult = this.performVADAnalysis(processedData, rmsLevel);
        if (this.onVADResult) {
            this.onVADResult(vadResult);
        }
        
        return processedData;
    }
    
    applyAutoGainControl(audioData) {
        const rms = this.calculateRMS(audioData);
        
        if (rms > 0) {
            // Calculate desired gain
            const desiredGain = this.targetRMS / rms;
            const clampedGain = Math.max(0.1, Math.min(10.0, desiredGain));
            
            // Update gain history for smoothing
            this.autoGainHistory.shift();
            this.autoGainHistory.push(clampedGain);
            
            // Apply smoothed gain
            const smoothedGain = this.autoGainHistory.reduce((a, b) => a + b) / this.autoGainHistory.length;
            
            // Apply gain to audio data
            for (let i = 0; i < audioData.length; i++) {
                audioData[i] *= smoothedGain;
            }
            
            // Update gain node
            this.gainNode.gain.setValueAtTime(smoothedGain, this.audioContext.currentTime);
        }
    }
    
    applyNoiseReduction(audioData) {
        // Simple spectral subtraction-based noise reduction
        // In production, you might want to use more sophisticated algorithms
        
        const alpha = 0.95; // Noise reduction factor
        
        for (let i = 0; i < audioData.length; i++) {
            const sample = audioData[i];
            const absSample = Math.abs(sample);
            
            if (absSample < this.noiseFloor * 2) {
                // Reduce noise in low-energy regions
                audioData[i] = sample * (1 - alpha);
            }
        }
    }
    
    performVADAnalysis(audioData, rms) {
        const timestamp = Date.now();
        
        // Calculate features
        const energy = rms;
        const zeroCrossings = this.calculateZeroCrossings(audioData);
        const spectralCentroid = this.calculateSpectralCentroid();
        
        // Update noise floor estimation
        this.updateNoiseFloor(energy);
        
        // Calculate speech probability
        const speechProbability = this.calculateSpeechProbability(
            energy, zeroCrossings, spectralCentroid
        );
        
        // Apply temporal logic
        const vadDecision = this.applyTemporalLogic(speechProbability, timestamp);
        
        // Update statistics
        if (vadDecision.isSpeech) {
            this.stats.speechFrames++;
        } else {
            this.stats.silenceFrames++;
        }
        
        return {
            isSpeech: vadDecision.isSpeech,
            confidence: vadDecision.confidence,
            energy: energy,
            speechProbability: speechProbability,
            features: {
                zeroCrossings: zeroCrossings,
                spectralCentroid: spectralCentroid,
                noiseFloor: this.noiseFloor
            },
            timestamp: timestamp
        };
    }
    
    calculateRMS(audioData) {
        if (audioData.length === 0) return 0;
        
        let sum = 0;
        for (let i = 0; i < audioData.length; i++) {
            sum += audioData[i] * audioData[i];
        }
        
        return Math.sqrt(sum / audioData.length);
    }
    
    calculateZeroCrossings(audioData) {
        if (audioData.length <= 1) return 0;
        
        let crossings = 0;
        for (let i = 1; i < audioData.length; i++) {
            if ((audioData[i] >= 0) !== (audioData[i - 1] >= 0)) {
                crossings++;
            }
        }
        
        return crossings;
    }
    
    calculateSpectralCentroid() {
        if (!this.analyserNode) return 0;
        
        const bufferLength = this.analyserNode.frequencyBinCount;
        const dataArray = new Uint8Array(bufferLength);
        this.analyserNode.getByteFrequencyData(dataArray);
        
        let weightedSum = 0;
        let magnitudeSum = 0;
        
        for (let i = 0; i < bufferLength; i++) {
            const magnitude = dataArray[i];
            const frequency = (i * this.audioContext.sampleRate) / (2 * bufferLength);
            
            weightedSum += frequency * magnitude;
            magnitudeSum += magnitude;
        }
        
        return magnitudeSum > 0 ? weightedSum / magnitudeSum : 0;
    }
    
    updateNoiseFloor(energy) {
        if (this.vadState === 'silence') {
            this.noiseEstimationSamples.push(energy);
            
            if (this.noiseEstimationSamples.length > this.maxNoiseEstimationSamples) {
                this.noiseEstimationSamples.shift();
            }
            
            if (this.noiseEstimationSamples.length >= 10) {
                // Use 75th percentile as noise floor estimate
                const sorted = [...this.noiseEstimationSamples].sort((a, b) => a - b);
                const index = Math.floor(sorted.length * 0.75);
                this.noiseFloor = sorted[index];
            }
        }
    }
    
    updateEnergyStatistics(energy) {
        this.energyHistory.shift();
        this.energyHistory.push(energy);
        
        this.stats.averageEnergy = this.energyHistory.reduce((a, b) => a + b) / this.energyHistory.length;
        this.stats.peakEnergy = Math.max(this.stats.peakEnergy, energy);
    }
    
    calculateSpeechProbability(energy, zeroCrossings, spectralCentroid) {
        // Energy-based probability
        const energyRatio = energy / (this.noiseFloor + 1e-10);
        const energyProb = Math.min(1.0, Math.max(0.0, (energyRatio - 1.0) / 10.0));
        
        // Zero crossing rate probability (speech has moderate ZCR)
        const zcrProb = 1.0 - Math.abs(zeroCrossings - this.config.zeroCrossingThreshold) / 50.0;
        const clampedZcrProb = Math.max(0.0, Math.min(1.0, zcrProb));
        
        // Spectral centroid probability (human speech frequency range)
        const specProb = (spectralCentroid > 300 && spectralCentroid < 3400) ? 1.0 : 0.5;
        
        // Weighted combination
        const totalProb = 0.6 * energyProb + 0.3 * clampedZcrProb + 0.1 * specProb;
        
        // Apply sensitivity adjustment
        const adjustedProb = totalProb * (2.0 - this.config.vadSensitivity);
        
        return Math.max(0.0, Math.min(1.0, adjustedProb));
    }
    
    applyTemporalLogic(speechProbability, timestamp) {
        const isProbableSpeech = speechProbability > this.config.vadSensitivity;
        
        switch (this.vadState) {
            case 'silence':
                if (isProbableSpeech) {
                    if (this.speechStartTime ==== null) {
                        this.speechStartTime = timestamp;
                    } else if (timestamp - this.speechStartTime >= this.config.minSpeechDuration) {
                        this.vadState = 'speech';
                        this.speechStartTime = safeGet(window, "initialValue", null);
                        this.lastSpeechTime = timestamp;
                        return { isSpeech: true, confidence: speechProbability };
                    }
                } else {
                    this.speechStartTime = safeGet(window, "initialValue", null);
                }
                break;
                
            case 'speech':
                if (isProbableSpeech) {
                    this.lastSpeechTime = timestamp;
                    this.silenceStartTime = safeGet(window, "initialValue", null);
                    return { isSpeech: true, confidence: speechProbability };
                } else {
                    if (this.silenceStartTime ==== null) {
                        this.silenceStartTime = timestamp;
                    } else if (timestamp - this.silenceStartTime >= this.config.minSilenceDuration) {
                        this.vadState = 'silence';
                        this.silenceStartTime = safeGet(window, "initialValue", null);
                    } else {
                        // Still in speech during short silence
                        return { isSpeech: true, confidence: speechProbability };
                    }
                }
                break;
        }
        
        return { isSpeech: false, confidence: speechProbability };
    }
    
    processBufferedAudio() {
        if (this.audioBuffer.length === 0) return;
        
        const now = Date.now();
        const chunkSamples = Math.floor(this.config.sampleRate * this.config.chunkDuration / 1000);
        
        if (this.audioBuffer.length >= chunkSamples) {
            // Extract chunk
            const chunk = new Float32Array(this.audioBuffer.splice(0, chunkSamples));
            
            // Send chunk for transcription
            if (this.onAudioChunk) {
                this.onAudioChunk(chunk, now);
            }
            
            this.lastChunkTime = now;
        }
    }
    
    updateConfig(newConfig) {
        Object.assign(this.config, newConfig);
        
        // Update gain node if sensitivity changed
        if (newConfig.vadSensitivity !=== null && this.gainNode) {
            // Adjust overall gain based on sensitivity
            const gainAdjustment = 1.0 + (1.0 - newConfig.vadSensitivity) * 0.5;
            this.gainNode.gain.setValueAtTime(gainAdjustment, this.audioContext.currentTime);
        }
        
        console.log('VAD configuration updated:', newConfig);
    }
    
    getStatistics() {
        const speechRatio = this.stats.speechFrames / Math.max(1, this.stats.totalFrames);
        
        return {
            ...this.stats,
            speechRatio: speechRatio,
            vadState: this.vadState,
            noiseFloor: this.noiseFloor,
            bufferSize: this.audioBuffer.length,
            isProcessing: this.isProcessing
        };
    }
    
    pause() {
        this.isProcessing = false;
        if (this.chunkInterval) {
            clearInterval(this.chunkInterval);
            this.chunkInterval = safeGet(window, "initialValue", null);
        }
        console.log('VAD processor paused');
    }
    
    resume() {
        this.isProcessing = true;
        this.chunkInterval = setInterval(() => {
            this.processBufferedAudio();
        }, this.config.chunkDuration);
        console.log('VAD processor resumed');
    }
    
    cleanup() {
        console.log('Cleaning up VAD processor...');
        
        this.isProcessing = false;
        
        if (this.chunkInterval) {
            clearInterval(this.chunkInterval);
            this.chunkInterval = safeGet(window, "initialValue", null);
        }
        
        if (this.scriptProcessor) {
            this.scriptProcessor.disconnect();
            this.scriptProcessor = safeGet(window, "initialValue", null);
        }
        
        if (this.analyserNode) {
            this.analyserNode.disconnect();
            this.analyserNode = safeGet(window, "initialValue", null);
        }
        
        if (this.gainNode) {
            this.gainNode.disconnect();
            this.gainNode = safeGet(window, "initialValue", null);
        }
        
        if (this.sourceNode) {
            this.sourceNode.disconnect();
            this.sourceNode = safeGet(window, "initialValue", null);
        }
        
        if (this.audioContext && this.audioContext.state !== 'closed') {
            this.audioContext.close();
            this.audioContext = safeGet(window, "initialValue", null);
        }
        
        if (this.mediaStream) {
            this.mediaStream.getTracks().forEach(track => track.stop());
            this.mediaStream = safeGet(window, "initialValue", null);
        }
        
        // Clear buffers
        this.audioBuffer = [];
        this.energyHistory = new Array(20).fill(0);
        this.noiseEstimationSamples = [];
        
        console.log('VAD processor cleanup complete');
    }
    
    // Static utility methods
    static async checkMicrophonePermission() {
        try {
            const result = await navigator.permissions.query({ name: 'microphone' });
            return result.state;
        } catch (issue) {
            console.warn('Unable to check microphone permission:', error);
            return 'unknown';
        }
    }
    
    static async requestMicrophonePermission() {
        try {
            const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
            stream.getTracks().forEach(track => track.stop());
            return true;
        } catch (issue) {
            console.warn('Microphone permission denied:', error);
            return false;
        }
    }
    
    static getSupportedConstraints() {
        return navigator.mediaDevices.getSupportedConstraints();
    }
}

// Export for use in other modules
window.VADProcessorAdvanced = VADProcessorAdvanced;

// Export as module if in Node.js environment
if (safeGet(arguments[0], "value") === null' && module.exports) {
    module.exports = VADProcessorAdvanced;
}

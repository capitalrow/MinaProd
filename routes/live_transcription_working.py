"""
CRITICAL: Working Live Transcription API Implementation
This replaces all non-functional transcription endpoints with a WORKING solution
"""

from flask import Blueprint, request, jsonify, current_app
from flask_login import current_user
from flask_socketio import emit
from werkzeug.utils import secure_filename
import os
import tempfile
import time
from datetime import datetime
from openai import OpenAI
import json

# Import socketio instance
from app import socketio

# Create blueprint
live_transcription_bp = Blueprint('live_transcription_working', __name__)

# Initialize OpenAI client
try:
    OPENAI_API_KEY = os.environ.get("OPENAI_API_KEY")
    if OPENAI_API_KEY:
        openai_client = OpenAI(api_key=OPENAI_API_KEY)
        print(f"[LIVE-API] ‚úÖ OpenAI client initialized successfully")
    else:
        openai_client = None
        print(f"[LIVE-API] ‚ùå OpenAI API key not found")
except Exception as e:
    openai_client = None
    print(f"[LIVE-API] ‚ùå OpenAI initialization failed: {e}")

@live_transcription_bp.route('/api/transcribe_chunk_streaming', methods=['POST', 'GET', 'OPTIONS'])
def transcribe_chunk_streaming():
    """
    CRITICAL: The main transcription endpoint that actually works
    Processes audio chunks and returns real transcription results
    """
    
    # Handle OPTIONS request for CORS
    if request.method == 'OPTIONS':
        return jsonify({'message': 'CORS preflight successful'}), 200
    
    # Handle GET request for testing
    if request.method == 'GET':
        return jsonify({
            'message': 'Transcription streaming endpoint is operational',
            'methods': ['POST', 'GET', 'OPTIONS'],
            'openai_configured': openai_client is not None,
            'endpoint': '/api/transcribe_chunk_streaming',
            'status': 'ready'
        })
    
    start_time = time.time()
    
    # Initialize variables early to avoid LSP errors
    chunk_id = '1'
    session_id = f'session_{int(time.time())}'
    
    try:
        print(f"[LIVE-API] üéµ Received transcription request")
        
        # Get form data
        audio_file = request.files.get('audio')
        session_id = request.form.get('session_id', session_id)
        chunk_id = request.form.get('chunk_id', chunk_id)
        is_final = request.form.get('is_final', 'false').lower() == 'true'
        
        # Check if OpenAI is available
        if not openai_client:
            return jsonify({
                'error': 'OpenAI API not configured',
                'text': '',
                'confidence': 0,
                'processing_time': time.time() - start_time,
                'chunk_id': chunk_id,
                'type': 'error'
            }), 500
        
        print(f"[LIVE-API] üìä Request details: audio_file={bool(audio_file)}, session_id={session_id}, chunk_id={chunk_id}")
        
        # Enhanced request logging
        if audio_file:
            print(f"[LIVE-API] üéµ Audio file: {audio_file.filename}, size: {audio_file.content_length or 'unknown'}")
        
        # Log all form data for debugging
        print(f"[LIVE-API] üìã Form data: {dict(request.form)}")
        print(f"[LIVE-API] üìÇ Files: {list(request.files.keys())}")
        
        if not audio_file:
            return jsonify({
                'error': 'No audio file provided',
                'text': '',
                'confidence': 0,
                'processing_time': time.time() - start_time,
                'chunk_id': chunk_id,
                'type': 'error'
            }), 400
        
        print(f"[LIVE-API] üìÅ Processing chunk {chunk_id}: {audio_file.filename} ({audio_file.content_length} bytes)")
        
        # Validate file size (minimum 1KB, maximum 25MB)
        if hasattr(audio_file, 'content_length') and audio_file.content_length:
            if audio_file.content_length < 1000:
                print(f"[LIVE-API] ‚ö†Ô∏è Chunk {chunk_id} too small: {audio_file.content_length} bytes")
                return jsonify({
                    'text': '',
                    'confidence': 0,
                    'processing_time': time.time() - start_time,
                    'chunk_id': chunk_id,
                    'type': 'partial',
                    'message': 'Audio chunk too small'
                })
            
            if audio_file.content_length > 25 * 1024 * 1024:  # 25MB limit
                return jsonify({
                    'error': 'Audio file too large (max 25MB)',
                    'text': '',
                    'confidence': 0,
                    'processing_time': time.time() - start_time,
                    'chunk_id': chunk_id,
                    'type': 'error'
                }), 413
        
        # Create temporary file for OpenAI processing
        with tempfile.NamedTemporaryFile(delete=False, suffix='.wav') as temp_file:
            audio_file.save(temp_file.name)
            temp_file_path = temp_file.name
        
        try:
            # Call OpenAI Whisper API
            print(f"[LIVE-API] ü§ñ Sending chunk {chunk_id} to OpenAI Whisper...")
            
            with open(temp_file_path, 'rb') as audio_data:
                transcription = openai_client.audio.transcriptions.create(
                    model="whisper-1",
                    file=audio_data,
                    response_format="verbose_json",
                    language="en"  # Can be made dynamic
                )
            
            # Extract text and confidence
            text = transcription.text.strip() if transcription.text else ""
            
            # Calculate confidence from segments if available
            confidence = 0.0
            if hasattr(transcription, 'segments') and transcription.segments:
                confidences = []
                for segment in transcription.segments:
                    if hasattr(segment, 'avg_logprob'):
                        # Convert log probability to confidence (0-1)
                        conf = min(1.0, max(0.0, (segment.avg_logprob + 1.5) / 1.5))
                        confidences.append(conf)
                
                if confidences:
                    confidence = sum(confidences) / len(confidences)
                else:
                    confidence = 0.5 if text else 0.0
            else:
                confidence = 0.5 if text else 0.0
            
            processing_time = time.time() - start_time
            
            print(f"[LIVE-API] ‚úÖ Chunk {chunk_id} transcribed successfully:")

            from services.transcription_service import TranscriptionService
            from app import socketio

            transcription_service = TranscriptionService()
            transcription_service._save_transcription_text(session_id, text, confidence)
            socketio.emit('update_dashboard', {'session_id': session_id, 'text': text})

            print(f"[LIVE-API]    Text: '{text[:100]}{'...' if len(text) > 100 else ''}'")
            print(f"[LIVE-API]    Confidence: {confidence:.3f}")
            print(f"[LIVE-API]    Processing time: {processing_time:.3f}s")
            
            # Prepare response
            response_data = {
                'text': text,
                'confidence': confidence,
                'processing_time': processing_time,
                'chunk_id': chunk_id,
                'session_id': session_id,
                'timestamp': datetime.utcnow().isoformat(),
                'type': 'success' if text else 'partial',
                'segments': []
            }
            
            # Add segment data if available
            if hasattr(transcription, 'segments') and transcription.segments:
                for segment in transcription.segments:
                    response_data['segments'].append({
                        'start': getattr(segment, 'start', 0),
                        'end': getattr(segment, 'end', 0),
                        'text': getattr(segment, 'text', ''),
                        'confidence': min(1.0, max(0.0, (getattr(segment, 'avg_logprob', -1) + 1.5) / 1.5))
                    })
            
            # Emit to WebSocket for real-time updates (if client connected)
            try:
                if text and confidence > 0.1:  # Only emit meaningful results
                    socketio.emit('transcription_result', response_data)
                    print(f"[LIVE-API] üì° Emitted transcription result via WebSocket")
            except Exception as e:
                print(f"[LIVE-API] ‚ö†Ô∏è WebSocket emit failed: {e}")
            
            return jsonify(response_data)
            
        finally:
            # Clean up temporary file
            try:
                os.unlink(temp_file_path)
            except Exception as e:
                print(f"[LIVE-API] ‚ö†Ô∏è Failed to delete temp file: {e}")
    
    except Exception as e:
        processing_time = time.time() - start_time
        error_msg = str(e)
        
        print(f"[LIVE-API] ‚ùå Transcription error for chunk {chunk_id}: {error_msg}")
        
        # Handle specific OpenAI errors
        if "rate_limit" in error_msg.lower():
            return jsonify({
                'error': 'Rate limit exceeded. Please wait a moment.',
                'text': '',
                'confidence': 0,
                'processing_time': processing_time,
                'chunk_id': chunk_id,
                'type': 'rate_limit'
            }), 429
        
        elif "quota" in error_msg.lower():
            return jsonify({
                'error': 'API quota exceeded. Please check your OpenAI account.',
                'text': '',
                'confidence': 0,
                'processing_time': processing_time,
                'chunk_id': chunk_id,
                'type': 'quota_exceeded'
            }), 402
        
        else:
            return jsonify({
                'error': f'Transcription failed: {error_msg}',
                'text': '',
                'confidence': 0,
                'processing_time': processing_time,
                'chunk_id': chunk_id,
                'type': 'error'
            }), 500

@live_transcription_bp.route('/api/transcription/health', methods=['GET'])
def transcription_health():
    """Health check for transcription service"""
    try:
        health_data = {
            'status': 'healthy',
            'openai_configured': openai_client is not None,
            'timestamp': datetime.utcnow().isoformat(),
            'version': '1.0.0'
        }
        
        # Test OpenAI connection if available
        if openai_client:
            try:
                # Quick test - list models (doesn't count against quota)
                models = openai_client.models.list()
                health_data['openai_status'] = 'connected'
                health_data['available_models'] = [m.id for m in models.data if 'whisper' in m.id]
            except Exception as e:
                health_data['openai_status'] = f'error: {str(e)}'
        
        return jsonify(health_data)
        
    except Exception as e:
        return jsonify({
            'status': 'unhealthy',
            'error': str(e),
            'timestamp': datetime.utcnow().isoformat()
        }), 500

# Test endpoint for debugging
@live_transcription_bp.route('/api/transcription/test', methods=['GET', 'POST'])
def transcription_test():
    """Test endpoint for debugging transcription issues"""
    if request.method == 'GET':
        return jsonify({
            'message': 'Transcription test endpoint is working',
            'openai_configured': openai_client is not None,
            'methods': ['GET', 'POST'],
            'timestamp': datetime.utcnow().isoformat()
        })
    
    else:  # POST
        return jsonify({
            'message': 'Test POST received',
            'form_data': dict(request.form),
            'files': list(request.files.keys()),
            'openai_ready': openai_client is not None
        })
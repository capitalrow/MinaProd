{
  "issues": {
    "chunks_processed_counter": {
      "element_id": "chunksProcessed",
      "issue": "Counter shows 0 even when chunks are being processed",
      "root_cause": "updateStats() may not be incrementing chunkCount properly",
      "fix_needed": "Ensure chunkCount increments on every successful chunk"
    },
    "latency_display": {
      "element_id": "latencyMs",
      "issue": "Shows 0ms despite processing taking 1200ms average",
      "root_cause": "Latency not being calculated/displayed correctly",
      "fix_needed": "Track and display actual processing latency"
    },
    "quality_score": {
      "element_id": "qualityScore",
      "issue": "Shows 0% despite having confidence scores",
      "root_cause": "Quality calculation not implemented or not updating UI",
      "fix_needed": "Map confidence scores to quality percentage"
    },
    "confidence_bars": {
      "element_id": "confidenceFill",
      "issue": "Progress bars not updating with confidence values",
      "root_cause": "updateConfidenceIndicators may have element selection issues",
      "fix_needed": "Fix element selection and bar width calculation"
    }
  },
  "javascript_fix": "\n/**\n * \ud83d\udd27 CRITICAL FIX: UI Metrics Update\n * This addresses the core issue where UI metrics show 0 values\n */\n\n// Add to FixedMinaTranscription class - enhanced updateStats method\nupdateStats(result) {\n    console.log('\ud83d\udd22 Updating UI stats with:', result);\n    \n    // \ud83d\udd27 FIX 1: Properly increment and display chunk count\n    const chunksElement = document.getElementById('chunksProcessed');\n    if (chunksElement) {\n        chunksElement.textContent = this.chunkCount;\n        console.log(`\ud83d\udcca Chunks processed: ${this.chunkCount}`);\n    }\n    \n    // \ud83d\udd27 FIX 2: Display actual latency from processing\n    const latencyElement = document.getElementById('latencyMs');\n    if (latencyElement && result.processing_time_ms) {\n        const latencyMs = Math.round(result.processing_time_ms);\n        latencyElement.textContent = `${latencyMs}ms`;\n        console.log(`\u26a1 Latency: ${latencyMs}ms`);\n    }\n    \n    // \ud83d\udd27 FIX 3: Calculate and display quality score\n    const qualityElement = document.getElementById('qualityScore');\n    if (qualityElement && result.confidence !== undefined) {\n        const qualityScore = Math.round(result.confidence * 100);\n        qualityElement.textContent = `${qualityScore}%`;\n        console.log(`\ud83c\udfaf Quality: ${qualityScore}%`);\n    }\n    \n    // Update word count (this seems to be working)\n    const words = this.cumulativeText.split(/\\s+/).filter(word => word.length > 0);\n    this.totalWords = words.length;\n    \n    if (this.elements.wordCount) {\n        this.elements.wordCount.textContent = this.totalWords;\n    }\n    \n    // Update accuracy/confidence (this seems to be working)\n    if (this.elements.accuracy) {\n        const confidence = Math.round((result.confidence || 0.95) * 100);\n        this.elements.accuracy.textContent = confidence + '%';\n    }\n    \n    // \ud83d\udd27 FIX 4: Update performance bars\n    this.updatePerformanceBars(result);\n}\n\n// \ud83d\udd27 NEW METHOD: Update performance bars with actual values\nupdatePerformanceBars(result) {\n    const updates = {\n        'confidenceFill': result.confidence ? Math.round(result.confidence * 100) : 0,\n        'latencyFill': result.processing_time_ms ? Math.min(Math.round((5000 - result.processing_time_ms) / 5000 * 100), 100) : 0,\n        'qualityFill': result.confidence ? Math.round(result.confidence * 100) : 0\n    };\n    \n    Object.entries(updates).forEach(([elementId, percentage]) => {\n        const element = document.getElementById(elementId);\n        if (element) {\n            element.style.width = `${percentage}%`;\n            console.log(`\ud83d\udcca ${elementId}: ${percentage}%`);\n        }\n    });\n    \n    // Update text values too\n    const textUpdates = {\n        'confidenceText': `${updates.confidenceFill}%`,\n        'latencyText': result.processing_time_ms ? `${Math.round(result.processing_time_ms)}ms` : '0ms',\n        'qualityText': `${updates.qualityFill}%`\n    };\n    \n    Object.entries(textUpdates).forEach(([elementId, text]) => {\n        const element = document.getElementById(elementId);\n        if (element) {\n            element.textContent = text;\n        }\n    });\n}\n\n// \ud83d\udd27 ENHANCED: processAudioChunk method to provide better data to updateStats\nasync processAudioChunk(audioBlob) {\n    try {\n        const startTime = Date.now();\n        \n        // ... existing code ...\n        \n        const latency = Date.now() - startTime;\n        \n        if (response.ok) {\n            const result = await response.json();\n            \n            // \ud83d\udd27 CRITICAL FIX: Add processing time to result object\n            result.processing_time_ms = latency;\n            result.chunk_size_bytes = audioBlob.size;\n            \n            // Enhanced text validation\n            if (result.text && result.text.trim() && \n                !result.text.includes('[No speech detected]') && \n                !result.text.includes('[Filtered]') &&\n                !result.text.includes('[Audio chunk too small]') &&\n                result.text.length > 1) {\n                \n                this.addTextToTranscript(result.text);\n                \n                // \ud83d\udd27 CRITICAL: Call updateStats with complete result object\n                this.updateUI(result);\n                this.updateConnectionStatus('processing');\n                \n                console.log(`\u2705 Transcribed: \"${result.text}\" (${latency}ms, confidence: ${Math.round((result.confidence || 0.9) * 100)}%)`);\n            } else {\n                console.log(`\u26a0\ufe0f No valid speech in chunk ${this.chunkCount} (${latency}ms)`);\n                \n                // \ud83d\udd27 NEW: Still update metrics even for failed chunks\n                this.updateChunkMetrics({\n                    processing_time_ms: latency,\n                    confidence: 0,\n                    chunk_size_bytes: audioBlob.size\n                });\n            }\n        }\n    } catch (error) {\n        console.error('\u274c Failed to process audio chunk:', error);\n        this.updateConnectionStatus('error');\n    }\n}\n\n// \ud83d\udd27 NEW METHOD: Update chunk metrics even for non-speech chunks\nupdateChunkMetrics(metrics) {\n    // Update chunks processed counter\n    const chunksElement = document.getElementById('chunksProcessed');\n    if (chunksElement) {\n        chunksElement.textContent = this.chunkCount;\n    }\n    \n    // Update latency even for failed chunks\n    const latencyElement = document.getElementById('latencyMs');\n    if (latencyElement && metrics.processing_time_ms) {\n        latencyElement.textContent = `${Math.round(metrics.processing_time_ms)}ms`;\n    }\n    \n    console.log(`\ud83d\udcca Metrics updated: chunk ${this.chunkCount}, latency ${Math.round(metrics.processing_time_ms)}ms`);\n}\n        ",
  "python_fix": "\n# \ud83d\udd27 BACKEND FIX: Ensure response includes all metrics needed by frontend\n# Add to routes/audio_http.py in the transcribe_audio function\n\ndef transcribe_audio():\n    request_start_time = time.time()\n    \n    # ... existing code ...\n    \n    # \ud83d\udd27 CRITICAL FIX: Always include timing metrics in response\n    processing_time_ms = (time.time() - request_start_time) * 1000\n    \n    # Successful transcription response\n    if clean_text and clean_text.strip():\n        response_data = {\n            'session_id': session_id,\n            'text': clean_text,\n            'confidence': confidence,\n            'chunk_number': chunk_number,\n            'is_final': is_final,\n            'status': 'success',\n            \n            # \ud83d\udd27 NEW: Include metrics needed by frontend\n            'processing_time_ms': processing_time_ms,\n            'audio_quality': quality_metrics.get('quality_score', 0.0),\n            'chunk_size_bytes': len(audio_bytes),\n            'server_timestamp': time.time()\n        }\n    else:\n        # No speech response - still include metrics\n        response_data = {\n            'session_id': session_id,\n            'text': '[No valid speech]',\n            'confidence': 0.0,\n            'chunk_number': chunk_number,\n            'is_final': is_final,\n            'status': 'no_speech',\n            \n            # \ud83d\udd27 NEW: Include metrics for UI updates\n            'processing_time_ms': processing_time_ms,\n            'audio_quality': quality_metrics.get('quality_score', 0.0),\n            'chunk_size_bytes': len(audio_bytes),\n            'reason': 'No valid speech detected'\n        }\n    \n    return jsonify(response_data)\n        ",
  "implementation_plan": {
    "step_1_identify_elements": {
      "description": "Verify all UI metric elements exist in HTML",
      "commands": [
        "grep -n \"chunksProcessed\\|latencyMs\\|qualityScore\" templates/live.html"
      ],
      "expected": "All metric element IDs found in template"
    },
    "step_2_update_javascript": {
      "description": "Apply JavaScript fixes to fixed_transcription.js",
      "file": "static/js/fixed_transcription.js",
      "changes": [
        "Enhance updateStats() method",
        "Add updatePerformanceBars() method",
        "Fix processAudioChunk() to pass metrics",
        "Add updateChunkMetrics() for non-speech chunks"
      ]
    },
    "step_3_update_backend": {
      "description": "Ensure backend provides all required metrics",
      "file": "routes/audio_http.py",
      "changes": [
        "Include processing_time_ms in all responses",
        "Include audio_quality metrics",
        "Include chunk_size_bytes for analysis"
      ]
    },
    "step_4_test_metrics": {
      "description": "Test metrics update in real-time",
      "test_procedure": [
        "1. Start recording session",
        "2. Speak for 10-15 seconds",
        "3. Verify all metrics update in real-time",
        "4. Check console logs for metric updates"
      ],
      "success_criteria": [
        "Chunks counter increments with each chunk",
        "Latency shows actual processing time",
        "Quality reflects confidence scores",
        "Progress bars update visually"
      ]
    }
  },
  "fix_summary": {
    "issue_analysis": {
      "root_cause": "Frontend updateStats() not properly updating all UI metric elements",
      "impact": "Users see 0 values for chunks, latency, quality despite successful processing",
      "severity": "HIGH - affects user experience and monitoring"
    },
    "solution_approach": {
      "frontend_fixes": [
        "Enhanced updateStats() method with proper element selection",
        "New updatePerformanceBars() method for visual indicators",
        "Improved processAudioChunk() to pass complete metrics",
        "New updateChunkMetrics() for comprehensive metric updates"
      ],
      "backend_enhancements": [
        "Include processing_time_ms in all responses",
        "Add audio quality metrics to responses",
        "Provide chunk size information for analysis"
      ]
    },
    "verification_steps": [
      "Test metrics update during recording",
      "Verify console logs show metric calculations",
      "Check UI elements receive proper values",
      "Validate metrics accuracy against backend logs"
    ]
  }
}